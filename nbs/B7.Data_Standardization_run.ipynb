{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21093d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from preprocess import *\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from bbsQt.model import kinect_utils as ku \n",
    "from bbsQt.model import rec_utils as ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e38b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "645fa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: 0이 아닌 점들에 대해서만.. \n",
    "# 좌-우 중 하나만 0이면 반대편도 같이 제외\n",
    "def shift_to_zero(skeleton, nframe=2, njoints=30):\n",
    "    early_frame_x = skeleton[:nframe*njoints:2]\n",
    "    early_frame_y = skeleton[1:nframe*njoints:2]\n",
    "\n",
    "    ix_nz = np.nonzero(early_frame_x)\n",
    "    iy_nz = np.nonzero(early_frame_y)\n",
    "\n",
    "    mean_x = np.mean(early_frame_x[ix_nz])\n",
    "    mean_y = np.mean(early_frame_y[iy_nz])\n",
    "    \n",
    "    skeleton[::2] -= mean_x \n",
    "    skeleton[1::2] -= mean_y \n",
    "    return skeleton\n",
    "\n",
    "from bbsQt.model import BBS_pp_utils as bbpp\n",
    "# 키 normalize \n",
    "## 어깨 - 팔꿈치 / 허벅지 / 종아리 \n",
    "common_joints = bbpp.COMMON_JOINT\n",
    "\n",
    "xy_joint_inds = dict([(name,i) for i, name in enumerate([prefix+cj for cj in common_joints for prefix in [\"x\", \"y\"]])])\n",
    "\n",
    "def joint_length(joint, j1:str, j2:str):\n",
    "    \"\"\"measure lentgh of a joint from j1 to j2\"\"\"\n",
    "    x1 = joint[xy_joint_inds[\"x\"+j1]]\n",
    "    x2 = joint[xy_joint_inds[\"x\"+j2]]\n",
    "    y1 = joint[xy_joint_inds[\"y\"+j1]]\n",
    "    y2 = joint[xy_joint_inds[\"y\"+j2]]\n",
    "    return np.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "    \n",
    "\n",
    "def arms_and_legs(skeleton, topk=2):\n",
    "    \"\"\"\n",
    "    get representative length of arms and legs\n",
    "    \"\"\"\n",
    "    lls = []\n",
    "    rls = []\n",
    "    las = []\n",
    "    ras = []\n",
    "    for sk in skeleton.reshape(-1, 30):\n",
    "        lls.append(joint_length(sk, 'l_hip', 'l_knee'))\n",
    "        rls.append(joint_length(sk, 'r_hip', 'r_knee'))\n",
    "        las.append(joint_length(sk, 'l_elbow', 'l_shoulder'))\n",
    "        ras.append(joint_length(sk, 'r_elbow', 'r_shoulder')) \n",
    "\n",
    "    out = []    \n",
    "    for arr in (lls, rls, las, ras):\n",
    "        out.append(arr[np.argsort(arr)[-topk]])\n",
    "    return out\n",
    "\n",
    "\n",
    "PAIRS = {'larm':('l_elbow', 'l_shoulder'),\n",
    "         'rarm':('r_elbow', 'r_shoulder'), \n",
    "         'lleg':('l_hip', 'l_knee'),\n",
    "         'rleg':('r_hip', 'r_knee'),\n",
    "         'body':('neck', 'pelvis')}\n",
    "\n",
    "def measure_lengths(skeleton, topk=2):\n",
    "    \"\"\"\n",
    "    get representative lengths of joints\n",
    "    \"\"\"\n",
    "    \n",
    "    out = []\n",
    "    for pair in PAIRS:\n",
    "        lengths = []\n",
    "        for sk in skeleton.reshape(-1, 30):\n",
    "            lengths.append(joint_length(sk, *PAIRS[pair]))\n",
    "            \n",
    "        out.append((pair,lengths[np.argsort(lengths)[-topk]]))\n",
    "            \n",
    "    return dict(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a7df5",
   "metadata": {},
   "source": [
    "# 스켈레톤이 아주 stochastic하기 때문에 5장 중 median으로 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0f61f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_npy(base_dir, action, nframe, frame_skip = 20, feature_ready=False, cam = \"e\"):\n",
    "    ### BBS data set\n",
    "    main_list = f\"whoismain/whoismain_{cam}_txt/main_list_{cam}_{action}.txt\"\n",
    "    npy_list = merge_main_npy(base_dir+main_list, prefix=base_dir + f\"npy_{cam}/\")\n",
    "\n",
    "    scene = np.load(npy_list[0]['npy'])\n",
    "    data = []\n",
    "    label=[]\n",
    "\n",
    "    for i, fn_npy in enumerate(npy_list):\n",
    "        if fn_npy['main'] in [0,1]:\n",
    "            try:\n",
    "            #if True:\n",
    "                scene = np.load(fn_npy['npy'])\n",
    "                for j in range(frame_skip):\n",
    "                    sub = smoothed_frame_N(scene, nframe=nframe, shift=j)\n",
    "\n",
    "                    # feature 기록\n",
    "                    rav_sub = ravel_rec(sub)\n",
    "                    data.append(rav_sub)\n",
    "                    label.append(fn_npy['score'])\n",
    "\n",
    "            except:\n",
    "                print(\"Main person was guessed, but not actually detected... \", fn_npy['npy'])\n",
    "\n",
    "\n",
    "    features = [ff for i in range(nframe) for ff in sub.dtype.names if ff not in \"frame\" ]\n",
    "    features = np.array(features)\n",
    "\n",
    "    data = np.stack(data)\n",
    "    label = np.array(label)\n",
    "    print(action, data.shape)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c69caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_data(data):\n",
    "    for joint in data:\n",
    "        joint = shift_to_zero(joint)\n",
    "\n",
    "    for dd in data:\n",
    "        body = measure_lengths(dd)\n",
    "        dd /= body['body'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf5fd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(data, label, action, cam, ntrees=20, max_depth = 7):\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(data, \n",
    "                                                                                label, \n",
    "                                                                                test_size=0.7, \n",
    "                                                                                stratify=label)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    # Use whole data to impose stricter boundary condition\n",
    "    X_total = np.concatenate((X_test, X_train))\n",
    "    scaler.fit(X_total)\n",
    "\n",
    "    X_train_normed = scaler.transform(X_train)\n",
    "    X_test_normed = scaler.transform(X_test)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=ntrees, max_depth=max_depth)\n",
    "    model.fit(X_train_normed, Y_train)\n",
    "    \n",
    "    pickle.dump(scaler, open(f\"scaler_{action}_{cam}.pickle\", 'wb'))\n",
    "    np.savetxt(f\"scaler_{action}_{cam}_scale.txt\", scaler.scale_, fmt='%.10f')\n",
    "    np.savetxt(f\"scaler_{action}_{cam}_min.txt\", scaler.min_, fmt='%.10f')    \n",
    "\n",
    "    pred = model.predict(X_test_normed)\n",
    "    print(\"정확도 :{0:.3f}\".format(accuracy_score(Y_test, pred)))\n",
    "\n",
    "    pickle.dump(model, open(f\"trained_model_{action}_{cam}.pickle\", \"wb\"))\n",
    "\n",
    "    data_to_save = {\"train_x\":X_train_normed, \n",
    "                     \"train_y\":Y_train,\n",
    "                     'valid_x':X_test_normed,\n",
    "                     'valid_y':Y_test}\n",
    "    pickle.dump(data_to_save, open(f\"BBS_dataset_{action}_{cam}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dfa4dc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 (8360, 240)\n",
      "정확도 :0.914\n"
     ]
    }
   ],
   "source": [
    "nframe = 8\n",
    "CAM_LIST= {1: \"e\",\n",
    "           2: \"e\",\n",
    "           3: \"a\",\n",
    "           4: \"e\",\n",
    "           5: \"e\",\n",
    "           6: \"e\",\n",
    "           7: \"e\",\n",
    "           8: \"a\",\n",
    "           9: \"a\",\n",
    "           10:\"e\",\n",
    "           11:\"e\",\n",
    "           12:\"e\",\n",
    "           13:\"a\",\n",
    "           14:\"e\"}\n",
    "\n",
    "# 3, 8, 9, 13\n",
    "for action in [12]:\n",
    "    cam = CAM_LIST[action]\n",
    "    data, label = load_all_npy(\"/home/hoseung/Work/data/BBS/\", action, nframe, cam=cam)\n",
    "    \n",
    "    std_data(data)\n",
    "    \n",
    "    ## Remove any nan\n",
    "    bad_row = np.unique(np.where(np.isnan(data))[0])\n",
    "    data = np.delete(data, bad_row, axis=0)\n",
    "    \n",
    "    label = np.array([ll for i,ll in enumerate(label) if not i in bad_row])\n",
    "    run_train(data, label, action, cam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
